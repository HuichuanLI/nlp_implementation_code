{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning BERT for downstream tasks\n",
    "So far we learned how to use the pre-trained BERT model. Now, let us learn how to finetune the pre-trained BERT for downstream tasks. Note that finetuning implies that we are not training BERT from scratch, instead, we are using the already trained BERT and updating its weights according to our task. \n",
    "\n",
    "In this section, we will learn how to finetune the pre-trained BERT for the following downstream tasks: \n",
    "\n",
    "- Text classification \n",
    "- Natural language inference \n",
    "- Named entity recognition \n",
    "- Question-answering\n",
    "\n",
    "Let us explore text classification in the next section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
